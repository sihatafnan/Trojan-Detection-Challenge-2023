import torch
from sentence_transformers.util import (semantic_search,
                    dot_score,
                    normalize_embeddings)
from torch.nn import CrossEntropyLoss
import numpy as np
from tqdm import tqdm
import random

from .baseline import TrojanDetector

# ============================== UTILS FOR PEZ ============================== #

def nn_project(curr_embeds, embedding_layer):
  batch_size, seq_len, emb_dim = curr_embeds.shape
  
  # Using the sentence transformers semantic search which is 
  # a dot product exact kNN search between a set of 
  # query vectors and a corpus of vectors
  curr_embeds = curr_embeds.reshape((-1,emb_dim))
  curr_embeds = normalize_embeddings(curr_embeds) # queries

  embedding_matrix = embedding_layer.weight
  embedding_matrix = normalize_embeddings(embedding_matrix) # corpus
  
  hits = semantic_search(curr_embeds, embedding_matrix, 
              query_chunk_size=curr_embeds.shape[0], 
              # top_k=3, # 
              top_k=5, # 1.531
              # top_k=100, # 
              score_function=dot_score
              )
  
  # import ipdb; ipdb.set_trace()

  nn_indices = torch.tensor([hit[0]["corpus_id"] for hit in hits], device='cuda')
  # Median policy: bad in 100
  # nn_indices = torch.tensor([hit[len(hit) // 2]["corpus_id"] for hit in hits], device='cuda')
  # Rand in 5 policy
  # nn_indices = torch.tensor([random.choice(hit)["corpus_id"] for hit in hits], device='cuda')
  projected_embeds = embedding_layer(nn_indices)
  projected_embeds = projected_embeds.reshape((batch_size, seq_len, emb_dim))
  nn_indices = nn_indices.reshape((batch_size, seq_len))

  return projected_embeds, nn_indices


# ============================== PEZ CLASS DEFINITION ============================== #

class PEZ(TrojanDetector):
  def __init__(self):
    super().__init__()

  def predict(self, targets, tokenizer, model, num_generate=20, batch_size=20, num_optim_tokens=30,
        num_steps=50, lr=1e-3, noise_scale=1e-3, verbose=False):
    """
    Generate predicted triggers for the provided targets

    :param num_generate: number of triggers to generate for each target
    :param batch_size: batch size for generating triggers
    """
    predictions = {}
    for i, target in tqdm(list(enumerate(targets))):
      if verbose:
        print(f"Generating triggers for target {i+1}/{len(targets)}: {target}")
      # break it down into batches
      num_batches = int(np.ceil(num_generate / batch_size))
      remainder = num_generate % batch_size
      current_predictions = []
      for j in range(num_batches):
        if verbose:
          print(f"Generating batch {j+1}/{num_batches}")
        current_batch_size = batch_size if (remainder == 0 or j < num_batches - 1) else remainder
        current_predictions += self._predict_single_target(target, tokenizer, model, current_batch_size, num_optim_tokens,
                                   num_steps, lr, noise_scale, verbose)
      predictions[target] = current_predictions
      # break
    return predictions

  def _predict_single_target(self, target, tokenizer, model, num_generate, num_optim_tokens,
                 num_steps, lr, noise_scale, verbose):
    """
    Generate predicted trigger for the provided target
    """
    class project_soft_embeds(torch.autograd.Function):
      """
      This is a PyTorch layer that projects the soft embeddings to the nearest
      hard embedding in the forward pass and passes the gradient through in the
      backward pass. This is a straight-through estimator.
      """
      @staticmethod
      def forward(ctx, input):
        """
        In the forward pass we receive a Tensor containing the input and return
        a Tensor containing the output. ctx is a context object that can be used
        to stash information for backward computation. You can cache arbitrary
        objects for use in the backward pass using the ctx.save_for_backward method.
        """
        ctx.save_for_backward(input)
        # projected_embeds, nn_indices = nn_project(input, model.transformer.wte)  # for GPT-2
        projected_embeds, nn_indices = nn_project(input, model.gpt_neox.embed_in)  # for Pythia
        return projected_embeds

      @staticmethod
      def backward(ctx, grad_output):
        """
        In the backward pass we receive a Tensor containing the gradient of the loss
        with respect to the output, and we need to compute the gradient of the loss
        with respect to the input.
        """
        input, = ctx.saved_tensors
        return grad_output  # straight-through estimator

    # ========== setup optim_embeds ========== #
    embedding_size = model.gpt_neox.embed_in.weight.shape[1]
    average_embed = model.gpt_neox.embed_in.weight.mean(0).data
    optim_embeds = torch.ones(num_optim_tokens, embedding_size).cuda() * average_embed.unsqueeze(0)
    optim_embeds = optim_embeds.unsqueeze(0).repeat(num_generate, 1, 1)  # add batch dimension
    optim_embeds += torch.randn_like(optim_embeds) * noise_scale  # add noise to initialization
    optim_embeds = torch.nn.Parameter(optim_embeds)
    optim_embeds.requires_grad_()

    # ========== setup target_embeds ========== #
    target_tokens = tokenizer(target, return_tensors="pt").to('cuda')
    target_embeds = model.gpt_neox.embed_in(target_tokens['input_ids']).data.squeeze(0)
    target_embeds = target_embeds.unsqueeze(0).repeat(num_generate, 1, 1)  # add batch dimension
    target_embeds.requires_grad_(False)

    # ========== setup optimizer and scheduler ========== #
    # optimizer = torch.optim.SGD([optim_embeds], lr=lr, weight_decay=0)
    # optimizer = torch.optim.Adam([optim_embeds], lr=lr, weight_decay=0)
    # optimizer = torch.optim.AdamW([optim_embeds], lr=lr, weight_decay=1e-2)
    optimizer = torch.optim.AdamW([optim_embeds], lr=lr, weight_decay=0)
    # optimizer = torch.optim.LBFGS([optim_embeds], lr=lr)

    # optimizer = torch.optim.Adam([optim_embeds], lr=lr, weight_decay=0)
    # optimizer = torch.optim.Adam([optim_embeds], lr=lr, weight_decay=1e-2)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_steps)

    # if 1:
    # optimizer = torch.optim.AdamW([optim_embeds], lr=lr, weight_decay=1e-2)
    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    #               optimizer, 'min', factor=0.975, patience=15, verbose=False)

    # from livelossplot import PlotLosses
    # from livelossplot.outputs import TensorboardLogger
    # plotlosses = PlotLosses(outputs=[TensorboardLogger()])
    # plotlosses = PlotLosses()

    # ========== run optimization ========== #
    for i in tqdm(range(num_steps)):
      # ========== compute logits with concatenated optim embeds and target text ========== #
      optim_embeds_projected = project_soft_embeds.apply(optim_embeds.half())  # assuming half-precision model

      # Add random noise
      # optim_embeds_projected = optim_embeds_projected + torch.randn_like(optim_embeds_projected) * noise_scale
      
      input_embeds = torch.cat([optim_embeds_projected, target_embeds], dim=1)
      
      outputs = model(inputs_embeds=input_embeds.half())  # assuming half-precision model
      logits = outputs.logits
      
      # ========== compute loss ========== #
      # Shift so that tokens < n predict n
      shift_logits = logits[..., num_optim_tokens-1:-1, :].contiguous()
      shift_labels = target_tokens['input_ids'].repeat(num_generate, 1)
      # Flatten the tokens
      # loss = None
      if  isinstance(optimizer, torch.optim.LBFGS):
        def loss_closure():
          optimizer.zero_grad()
          loss_fct = CrossEntropyLoss()
          loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
          loss.backward(inputs=[optim_embeds])
          return loss
      else:
        
        loss_fct = CrossEntropyLoss()
        # loss_fct = CrossEntropyLoss(label_smoothing=0.1)
        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))

        # loss_fct = torch.nn.KLDivLoss()
        # loss_fct = torch.nn.KLDivLoss(redu/ction='batchmean')
        # t1 = shift_logits.view(-1, shift_logits.size(-1))
        # t2 = shift_labels.view(-1, 1)
        # t1 = torch.nn.functional.log_softmax(t1, dim=-1)
        # t2 = torch.nn.functional.log_softmax(t2, dim=-1).squeeze(dim=1)

        # loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
        # loss = loss_fct(t1, t2.half())
        # import ipdb; ipdb.set_trace()
    

        optimizer.zero_grad()
        loss.backward(inputs=[optim_embeds])

      # if loss.item() < 0.5:
      #     print('early stopping')
      #     break
      
      # ========== update optim_embeds ========== #
      
      if  isinstance(optimizer, torch.optim.LBFGS):
        loss = optimizer.step(loss_closure)
      else:
        optimizer.step()
      
      if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
        scheduler.step(loss.item())
      else:
        scheduler.step()

      if verbose:
        if i % 10 == 0:
          print('\n{} -> {:.3f},'.format(i, loss.item()), end='', flush=True)
          # plotlosses.update({'loss': loss.item()})
          # plotlosses.send()
          if i % 100 == 0: print()
    
    if verbose:
      print()
      print('{} {:.3f}'.format(i, loss.item()))

    # ========== detokenize and print the optimized prompt ========== #
    _, nn_indices = nn_project(optim_embeds.half(), model.gpt_neox.embed_in)

    optim_prompts = tokenizer.batch_decode(nn_indices)
    if verbose:
      print('target_text:', target)
      for i, p in enumerate(optim_prompts):
        print(f'optim_prompt {i}:', p)
    
    return optim_prompts
